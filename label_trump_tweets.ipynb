{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f757aee6",
   "metadata": {},
   "source": [
    "# Label Trump Tweets\n",
    "\n",
    "> Before you start, make sure you've followed the setup instructions in the [README](README.md).\n",
    "\n",
    "Ingest a CSV of Trump's tweets, downloaded from Junkipedia.org, and use Ollama to identify which posts are text-based vs image/media-based.\n",
    "\n",
    "After completing this basic exercise, you can expand the analysis to perform other classification tasks, such as:\n",
    "\n",
    "* Assign sentiment (positive, negative, neutral)\n",
    "* Assign one or more topics (free-form)\n",
    "* Assign one of a set of pre-set categories (e.g. politics, business, etc.)\n",
    "* Perform entity extraction\n",
    "\n",
    "## Ways to experiment\n",
    "\n",
    "Experiment with different prompts to see how well a local model can classify the tweets.\n",
    "\n",
    "- One big prompt that tries to do everything\n",
    "- Multiple smaller prompts that do one thing each\n",
    "\n",
    "As you perform the above work, store the results in a way that lets you link the LLM results with the original data. You can do this using a simple dictionary (demonstrated below), or by adding new columns (e.g. `text_based`, `sentiment`, `topics`, `category`). \n",
    "\n",
    "You may need multiple versions of each column type to capture the different results from different prompts. For example, you might have `sentiment_v1`, `sentiment_v2`, etc.\n",
    "\n",
    "## Prompt Templates\n",
    "\n",
    "You should be using chat [Prompt Templates](https://python.langchain.com/docs/concepts/prompt_templates/) as part of your workflow, as that will allow you to easily inject the text of posts into the prompts, for example in the context of a `for` loop.\n",
    "\n",
    "*See LangChain's [Prompt Templates guide](https://python.langchain.com/docs/how_to/#prompt-templates) for more details.*\n",
    "\n",
    "## Structured output\n",
    "\n",
    "As your prompts get more sophisticated and the resulting output more complex, you may also want to use LangChain's [structured output](https://python.langchain.com/docs/how_to/structured_output/) feature to parse the results into a structured format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c0e86a",
   "metadata": {},
   "source": [
    "## Ingest the data\n",
    "\n",
    "We'll start with some basic imports and then load the CSV file containing Trump's tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a82b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac68558",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"trump_tweets_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b641003b",
   "metadata": {},
   "source": [
    "Review the data to see what columns are available. You can use the `head()` method to view the first few rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dacdf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20527c",
   "metadata": {},
   "source": [
    "## Classify tweets by type\n",
    "\n",
    "Now we'll demonstrate how to classify Tweets as text-based or image/media-based using Ollama's LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0198d0b1",
   "metadata": {},
   "source": [
    "Prepare a [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html#chatprompttemplate) for classifying tweets as text-based or media-based. This template will be used to generate prompts for the LLM, and allows you to easily inject the text of each tweet into the prompt.\n",
    "\n",
    "- The `system` message below gives the LLM a \"persona\" -- a common strategy used to optimize the results of an LLM interaction -- and a set of instructions on how to classify the tweets. \n",
    "- The `user` message will contain the tweet text that should be classified. It uses curly braces to templatize the text (`{tweet_text}`), in a way that is reminiscent of Python [\"f\"-strings](https://docs.python.org/3/reference/lexical_analysis.html#f-strings).\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf03210",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are helpful assistant that classifies tweets. For each tweet, determine if it is: \"\n",
    "        \" - Text-only - i.e. there is only text and no images or videos\\n\"\n",
    "        \" - Multimedia-only - i.e. If there is only a link that starts with 'https://t.co\\n\"\n",
    "        \" - Text and Multimedia - i.e. there is both text and at least on image or video\\n\\n\"\n",
    "        \"Return a single word classification for each tweet: 'text', 'multimedia', or 'both'.\"\n",
    "        \"Make sure your 1-word classification is in lowercase and does not include any punctuation.\"\n",
    "    ),\n",
    "    (\"user\", \"{tweet_text}\"),\n",
    "])\n",
    "\n",
    "chat = OllamaLLM(model=\"gemma3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65943aa",
   "metadata": {},
   "source": [
    "Test a small subset of tweets to verify that the prompts are working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292613bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the results of the classification\n",
    "data = {}\n",
    "# We'll use a counter to limit the number of tweets processed, to speed up testing\n",
    "counter = 0\n",
    "# Loop through the DataFrame and classify each tweet\n",
    "for index, row in df.iterrows():\n",
    "    # Limit the number of tweets processed to 5 for testing. \n",
    "    if counter == 5:\n",
    "        break\n",
    "    post_id = row['PostId']\n",
    "    post_text = row['post_body_text']\n",
    "    # Generate the prompt by \"injecting\" the tweet text into the prompt\n",
    "    compiled_prompt = prompt.format(tweet_text=post_text)\n",
    "    # Call the model with the prompt\n",
    "    # Note: The model may take a few seconds to respond, \n",
    "    # depending on the size of the model and the complexity of the prompt\n",
    "    response = chat.invoke(compiled_prompt)\n",
    "    # Store the response in the dictionary using the post_id as the key\n",
    "    data[post_id] = {\n",
    "        'post_id': post_id,\n",
    "        'post_body_text': post_text,\n",
    "        'classification': response.strip()\n",
    "    }\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58e1da",
   "metadata": {},
   "source": [
    "Now print and review the results. As you do this, consider the following questions:\n",
    "\n",
    "- Did the model classify the tweets correctly?\n",
    "- Did the \"label\" column contain the expected values (\"text\" or \"media\")? Or did it contain other values (e.g. \"Text\" or \"MEDIA\" or \"text-and-media\")?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05c75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6184af",
   "metadata": {},
   "source": [
    "## Keep experimenting\n",
    "\n",
    "Depending on the results of your initial tests, you may want to try different prompts or prompt templates. \n",
    "\n",
    "Specifically, you'll want to experiment with different ways of phrasing the classification task in the `system` message. \n",
    "\n",
    "You can also try different models, such as Llama 2 or Mistral, to see if they produce better results. To do this, you'll need to use Ollama to install the models  you want to test, and then update the `model` parameter in the `OllamaLLM` class to use the new model. For example, to use Deepseek, you would change the model name from `gemma3` to `deepseek-llm`.\n",
    "\n",
    "You can search for available models on the [Ollama website](https://ollama.com/search).\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
